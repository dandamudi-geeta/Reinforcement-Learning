{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBHku1wfPl0SGvX9WsS67r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dandamudi-geeta/Reinforcement-Learning/blob/main/2348512_RL(Lab4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OGskW4cwhxiS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MDP:\n",
        "    def __init__(self, states, actions, transition_probabilities, rewards):\n",
        "        self.states = states      # List of states\n",
        "        self.actions = actions    # List of actions\n",
        "        self.transition_probabilities = transition_probabilities  # dict: P(state'|state, action)\n",
        "        self.rewards = rewards      # dict: reward(state, action)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return self.actions\n",
        "\n",
        "    def get_next_states(self, state, action):\n",
        "        return list(self.transition_probabilities[state][action].keys())"
      ],
      "metadata": {
        "id": "d1i68EK7qWsx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy:\n",
        "    def __init__(self, states, actions):\n",
        "        self.policy = {state: np.random.choice(actions) for state in states}\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return self.policy[state]\n",
        "\n",
        "    def update(self, state, action):\n",
        "        self.policy[state] = action\n",
        "\n",
        "class ValueFunction:\n",
        "    def __init__(self, states):\n",
        "        self.values = {state: 0.0 for state in states}\n",
        "\n",
        "    def get_value(self, state):\n",
        "        return self.values[state]\n",
        "\n",
        "    def set_value(self, state, value):\n",
        "        self.values[state] = value"
      ],
      "metadata": {
        "id": "lT4hMk6fqWvY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(mdp, policy, theta=0.001):\n",
        "    value_func = ValueFunction(mdp.states)\n",
        "    while True:\n",
        "        delta = 0.0\n",
        "        for state in mdp.states:\n",
        "            v = value_func.get_value(state)\n",
        "            action = policy.select_action(state)\n",
        "            value_func.set_value(state, sum(\n",
        "                mdp.transition_probabilities[state][action][next_state] *\n",
        "                (mdp.rewards[state][action] + value_func.get_value(next_state))\n",
        "                for next_state in mdp.get_next_states(state, action)\n",
        "            ))\n",
        "            delta = max(delta, abs(v - value_func.get_value(state)))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return value_func"
      ],
      "metadata": {
        "id": "z7JgH2PMqWyD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(mdp, value_func, policy):\n",
        "    policy_stable = True\n",
        "    for state in mdp.states:\n",
        "        old_action = policy.select_action(state)\n",
        "        # pick the action that maximizes the value function\n",
        "        action_values = {}\n",
        "        for action in mdp.get_actions(state):\n",
        "            action_values[action] = sum(\n",
        "                mdp.transition_probabilities[state][action][next_state] *\n",
        "                (mdp.rewards[state][action] + value_func.get_value(next_state))\n",
        "                for next_state in mdp.get_next_states(state, action)\n",
        "            )\n",
        "        best_action = max(action_values, key=action_values.get)\n",
        "        policy.update(state, best_action)\n",
        "        if old_action != best_action:\n",
        "            policy_stable = False\n",
        "    return policy_stable\n"
      ],
      "metadata": {
        "id": "VTE7_p8wqW0V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(mdp):\n",
        "    policy = Policy(mdp.states, mdp.actions)\n",
        "    while True:\n",
        "        value_func = policy_evaluation(mdp, policy)\n",
        "        policy_stable = policy_improvement(mdp, value_func, policy)\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy\n"
      ],
      "metadata": {
        "id": "Etb-bVTtqwup"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "states = ['s1', 's2', 's3']\n",
        "actions = ['a1', 'a2']\n",
        "transition_probabilities = {\n",
        "    's1': {'a1': {'s1': 0.8, 's2': 0.2}, 'a2': {'s1': 0.5, 's3': 0.5}},\n",
        "    's2': {'a1': {'s1': 0.1, 's3': 0.9}, 'a2': {'s2': 1.0}},\n",
        "    's3': {'a1': {'s1': 1.0}, 'a2': {'s2': 1.0}}\n",
        "}"
      ],
      "metadata": {
        "id": "95GNxFG5qecE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = {\n",
        "    's1': {'a1': 0, 'a2': 1},\n",
        "    's2': {'a1': 0, 'a2': 0},\n",
        "    's3': {'a1': 0, 'a2': 0}\n",
        "}\n"
      ],
      "metadata": {
        "id": "XOyN2-N8qeek"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = MDP(states, actions, transition_probabilities, rewards)\n",
        "optimal_policy = policy_iteration(mdp)"
      ],
      "metadata": {
        "id": "vNVsp4CNqeh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the optimal policy\n",
        "for state in states:\n",
        "    print(f\"Optimal action for {state}: {optimal_policy.select_action(state)}\")\n"
      ],
      "metadata": {
        "id": "mdwxjemUqqpg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}